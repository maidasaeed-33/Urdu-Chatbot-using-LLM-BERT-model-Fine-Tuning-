{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ayohfLiB42V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from tqdm import tqdm\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section imports necessary libraries and modules:\n",
        "- `numpy` and `pandas` for data manipulation.\n",
        "- `re` for regular expressions.\n",
        "- `torch` for building and training neural networks.\n",
        "- `random` for random number generation.\n",
        "- `torch.nn` for neural network components.\n",
        "- `BertTokenizer` and `BertModel` from the Hugging Face `transformers` library, used for working with BERT models.\n",
        "- `tqdm` for displaying progress bars during training.\n",
        "- `os` for operating system related functions.\n"
      ],
      "metadata": {
        "id": "er_6jVBqLzZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dropout = nn.Dropout(0.1)  # Adjust dropout rate\n",
        "        # Add an additional layer\n",
        "        self.fc1 = nn.Linear(768, len(answers))\n",
        "\n",
        "    def forward(self, sent_id, attention_mask):  # Update method signature to accept attention_mask\n",
        "        cls_hs = self.bert(sent_id, attention_mask=attention_mask)[0][:, 0]\n",
        "        x = self.dropout(cls_hs)\n",
        "        output = self.fc1(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "4b_IlPplCEuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, a custom neural network architecture `BERT_Arch` is defined. It inherits from `nn.Module` and implements the BERT architecture with an additional linear layer for classification."
      ],
      "metadata": {
        "id": "g-3xiFORL3lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the multilingual BERT model\n",
        "bert = BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
        "\n",
        "# Load the multilingual BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYl89HO-DSmp",
        "outputId": "4253700d-c6b9-427e-8054-e969eb19bcb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section loads the pre-trained BERT model and tokenizer. It uses the `'bert-base-multilingual-uncased'` version."
      ],
      "metadata": {
        "id": "30WwNN48L7ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "SEJc-q_dCH7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determines whether to use GPU or CPU for computation based on availability."
      ],
      "metadata": {
        "id": "WFb3pmxrL-Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !huggingface-cli login"
      ],
      "metadata": {
        "id": "oSCPvt0dCtHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    questions = df['questions'].tolist()\n",
        "    answers = df['answers'].tolist()\n",
        "    return questions, answers\n",
        "\n",
        "questions, answers = load_dataset('dataset.csv')"
      ],
      "metadata": {
        "id": "7dG01N4ACRev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function loads the dataset from a CSV file and returns lists of questions and answers."
      ],
      "metadata": {
        "id": "In_eRdT4MBfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and encode questions\n",
        "max_seq_len = 55  # or any suitable value\n",
        "tokens_train = tokenizer(\n",
        "    questions,\n",
        "    max_length=max_seq_len,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCOVbFCtCPwe",
        "outputId": "6e22eb42-dcc1-45f8-eed1-524cf80010fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizes and encodes the questions using the BERT tokenizer, ensuring they are of uniform length by padding/truncating."
      ],
      "metadata": {
        "id": "Qq8fJtpBMEAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to tensors\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor([answers.index(ans) for ans in answers])\n"
      ],
      "metadata": {
        "id": "oD2sNWG9ML1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converts the tokenized data into PyTorch tensors."
      ],
      "metadata": {
        "id": "DxJu4wBuMGfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "# define a batch size\n",
        "batch_size = 64\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# DataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "J3hGhWRjMT9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part prepares the data for training by creating a `DataLoader` object."
      ],
      "metadata": {
        "id": "NVW8vwYHMRaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize your model\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# Push the model to GPU\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "GbS00et0MbQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializes the BERT model and sends it to the appropriate device (GPU or CPU)."
      ],
      "metadata": {
        "id": "iEw9KZI3MYa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)  # Adjust learning rate\n",
        "\n",
        "# Loss function\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "C3DpdLTUCOD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines the optimizer (Adam) and the loss function (CrossEntropyLoss)."
      ],
      "metadata": {
        "id": "mSrOOtxPMg00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
        "        batch = [r.to(device) for r in batch]\n",
        "        sent_id, mask, labels = batch\n",
        "        model.zero_grad()\n",
        "        output = model(sent_id, mask)\n",
        "        loss = loss_function(output, labels)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    return avg_loss\n"
      ],
      "metadata": {
        "id": "ghoscuNPCLMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function defines the training loop."
      ],
      "metadata": {
        "id": "0mgdtXI3MizD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of training epochs\n",
        "epochs = 3  # Increase for better training"
      ],
      "metadata": {
        "id": "0oGgzMrPCKOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for epoch in range(epochs):\n",
        "    print(f'\\n Epoch {epoch + 1} / {epochs}')\n",
        "    train_loss = train()\n",
        "    print(f'Training Loss: {train_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozoDigVUCJX1",
        "outputId": "b4c43959-ea48-4017-a4f3-611c420d3322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2/2 [00:00<00:00,  2.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.404\n",
            "\n",
            " Epoch 2 / 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2/2 [00:00<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.412\n",
            "\n",
            " Epoch 3 / 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 2/2 [00:00<00:00,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trains the model for a specified number of epochs."
      ],
      "metadata": {
        "id": "Nwdw2VJhMlRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'trained_model.pth')"
      ],
      "metadata": {
        "id": "VxmNaGhGEX2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saves the trained model to disk."
      ],
      "metadata": {
        "id": "v_MLqPdZMne7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model_path = \"/content/trained_model.pth\"\n",
        "model = BERT_Arch(xlm_roberta)\n",
        "\n",
        "# Map the model to CPU device\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "4wfDurEUEZpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will load the save model"
      ],
      "metadata": {
        "id": "KShSjA5uMqXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(input_str, model):\n",
        "    # Remove non-Arabic characters and spaces\n",
        "    input_str = re.sub(r'[^\\u0600-\\u06ff\\s]+', '', input_str)\n",
        "\n",
        "    # Tokenize the input string\n",
        "    tokens_test_data = tokenizer(\n",
        "        [input_str],\n",
        "        max_length=max_seq_len,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_token_type_ids=False\n",
        "    )\n",
        "\n",
        "    # Convert the tokenized text to tensors\n",
        "    test_seq = torch.tensor(tokens_test_data['input_ids']).to(device)\n",
        "    test_mask = torch.tensor(tokens_test_data['attention_mask']).to(device)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient calculation to improve efficiency\n",
        "    with torch.no_grad():\n",
        "        # Get model predictions\n",
        "        preds = model(test_seq, attention_mask=test_mask)\n",
        "        # Apply softmax function to get probabilities\n",
        "        preds = torch.softmax(preds, dim=1)\n",
        "        # Get the index of the highest probability\n",
        "        pred_idx = torch.argmax(preds, dim=1).item()\n",
        "\n",
        "    # Return the predicted answer\n",
        "    return answers[pred_idx]"
      ],
      "metadata": {
        "id": "4O3QPWfOEbCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines a function to get predictions from the model given an input question."
      ],
      "metadata": {
        "id": "Tj3_lHt0MuK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatbot loop\n",
        "\n",
        "while True:\n",
        "    input_question = input(\"You: \")\n",
        "    predicted_answer = get_prediction(input_question, model)\n",
        "    print(f\"Chatbot: {predicted_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "olS2aUkuEcLu",
        "outputId": "5f778980-1b35-46e5-ad08-744e85f476d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: کیا آپ زندہ ہیں\n",
            "Chatbot: میں ایک چیٹ روبوٹ ہوں، میری جان انٹرنیٹ پر موجود ہوتی ہے۔\n",
            "You: کیا آپ حیات ہیں\n",
            "Chatbot: میں ایک چیٹ روبوٹ ہوں، میری جان انٹرنیٹ پر موجود ہوتی ہے۔\n",
            "You: کیا آپ کو خوشی ہوتی ہے؟\n",
            "Chatbot: میری کوئی خواہش نہیں ہوتی، میں صرف انتظامیہ کرتا ہوں۔\n",
            "You: کیا آپ کو مسکراہٹ ہوتی ہے؟\n",
            "Chatbot: مجھے بھوک نہیں لگتی، لیکن آپ میری مدد کر سکتے ہیں۔\n",
            "You: اپ کا ویٹ کتنا ہے\n",
            "Chatbot: میرے جیسے چیٹ روباٹ کا کوئی رنگ نہیں ہوتا، میں صرف موجودہ معلومات فراہم کرتا ہوں۔\n",
            "You: کیا آپ کو پیار ہوتا ہے\n",
            "Chatbot: میری قوانین کی روشنی میں، میرے لیے محبت کا معنی نہیں ہوتا۔\n",
            "You: کیا آپ کو محبت ہوتا ہے\n",
            "Chatbot: میری کوئی خواہش نہیں ہوتی، میں صرف انتظامیہ کرتا ہوں۔\n",
            "You: کیا آپ کو پیار ہوتا ہے\n",
            "Chatbot: میری قوانین کی روشنی میں، میرے لیے محبت کا معنی نہیں ہوتا۔\n",
            "You: آپ کہاں سے ہیں؟\n",
            "Chatbot: میں انٹرنیٹ پر موجود ہوتا ہوں۔\n",
            "You: آپ کو کونسی غذا پسند ہے\n",
            "Chatbot: میرے جیسے چیٹ روباٹ کو کوئی غذا پسند نہیں ہوتی، میں خوراک کا محسوس نہیں کرتا۔\n",
            "You: آپ کو کونسی کھانا پسند ہے\n",
            "Chatbot: میرے جیسے چیٹ روباٹ کو کوئی غذا پسند نہیں ہوتی، میں خوراک کا محسوس نہیں کرتا۔\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c962e6a4b304>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Chatbot loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minput_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpredicted_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_question\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Chatbot: {predicted_answer}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates an interactive loop where the user can input questions to the chatbot, and it responds with predicted answers."
      ],
      "metadata": {
        "id": "CeVvZeLPMwO9"
      }
    }
  ]
}